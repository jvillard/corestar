In a moment of insanity, I, Radu Grigore, decided to throw away lots of
perfectly functioning code, and replace it with some other lot of perfectly
functioning code.  Since the beginning, coreStar had dealt with equalities,
disequalities, uninterpreted functions, and related matters by its own.
Specifically, it had three modules (Congruence, Cterm, Clogic), the first of
which was very complicated, and the other twos heaping on top of it.  For
efficiency, it represented formulas in a very complicated way, which made it
difficult to perform operations such as ‘build P*Q out of P and Q’.  We (Rasmus
and I) tried to remedy the situation by implementing the said operations.  In
the process, we fixed several glaring bugs in the old code, and replaced them
with subtle bugs, which we were subsequently unable to repair.

Needless to say, I consider this state of affairs as unsatisfactory.

So, here is the plan, in brief format:  Discharge anything that has to do with
equalities and uninterpreted functions to an SMT solver, and keep the formula
representation inside coreStar as simple as possible.  Don't worry about
efficiency; fix it later if necessary.

Now, a bit more detail.  The main data type that we must define is that for
expressions.  I plan to use these to represent several things:
  - terms: 0, 1, x, 1 + x, ...
  - formulas: x == y,  x.f |-> y,  x == y * y != z
  - patterns:  _x.next |-> _y * _y.next |-> _x
These share several common operations:
  - construction:  the operators used in patterns are the same as those used in
    formulas and terms, although operators used in formulas are disjoint from
    those used in terms
  - convert to normal forms, such as the form expected by the prover; a normal
    form could be subtype, but I'd prefer just semantic checks (e.g.
    is_normal_form_foo : formula -> bool)
Roughly, we would need to distinguish several types of operators, such as
  - those used in terms: +, -, ...
  - equality and disequality (they are special because pattern matching is done
    modulo equality; see later)
It is, however, not completely clear to me how many categories we should have.
For example, are spatial predicates treated mostly the same or mostly different
from non-spatial (pure) predicates?  In the former case, you'd not want a
distinction at the type level; in the latter case, you would.

*Remark:*  The previous paragraph raises the question of how much information to
encode in types.  My personal preference appears below.  I'm unsure whether what
I propose is too little or too much.  I suspect most people (that means you)
would think ‘too little’.  It seems to me there is no such thing as the ‘right
amount’, but there is a reasonable range, and the most important thing is to not
go overboard.  One reason why I tilt towards the ‘too little’ end of the range
is that I'm uncertain what are the right categories of operators, and code that
uses types less feels more malleable.

There are also operations that apply only to some particular kinds of
expressions, such as:
  - terms and formulas: conversion to the SMT format (I think there's no such
    thing as a pattern in SMT)
  - terms/formulas vs pattern: matching

*TODO:* Since we don't want to handle equalities and since pattern matching is
done modulo equalities, we must use Z3 during pattern matching.  We need to
clarify how this is done.


--------------------------------------------------------------------------------
- Formula representation

The two main constructors are
  make_app : operator → formula list → formula
  make_var : string → formula
and the accompanying deconstructors are
  break_app : formula → (operator * formula list)
  break_var : formula → string

*Why:*  Of course, this could be just a (transparent) variant type.  Why go
through the trouble to hide it behind constructors/deconstructors?  I think it's
likely that we would have to implement hash-consing [1], such that we can
determine in O(1) time whether two formulas/terms are syntactically equal.  The
old implementation was doing this by using some integer identifiers in
Congruence, which had a limited scope, meaning that it was *sometimes* possible
to compare terms quickly.

Structural equality should be tested using the function
  eq : formula → formula → bool
rather than the OCaml's polymorphic comparison =.  This way, it will be easier
to switch to a new implementation in the future.

*TODO:* I'm not sure whether [operator] should be just a string, or a variant
that identifies the operators.

Some other helpers need to be provided.  For example, instead of
  make_app Star [a; b]
it's more convenient to say
  make_star a b
But, such conveniences should be easy to implement in terms of the above.

*Why:*  Destructors are supposed to make it reasonably convenient to use pattern
matching, even if the variant type used for formulas is not transparent.


--------------------------------------------------------------------------------
- Sequent representation

The coreStar sequents have the shape
  R | P ⊢ Q
and the semantics
  if h ⊨ P * R, then h ⊨ Q * R
for some definition of ⊨.

*TODO:* I say ‘some’ because I'm not sure whether we assume the intuitionistic
definition in some essential way.  (In the past I simply assumed the
intuitionistic definition when thinking about it, but other users, such as
Jules, probably aren't.  Another way to put it: While modifying the alt-abd
branch I didn't pay attention to whether those modifications would make life
hard for Jules. Sorry.)

I *think* it might be fine to have a transparent type
  type sequent =
    { sequent_frame : formula
    ; sequent_hypothesis : formula
    ; sequent_consequence : formula }

--------------------------------------------------------------------------------
-- Variables scope

There are two kinds of variables in the logic, pvars (aka program variables) and
lvars (aka logical variables). These have different scoping conventions.
Variables occur in triples, sequents, and confgraphs.

The scope of an lvar is, respectively, the triple, the sequent, or the
confgraph. One could think of lvars as being universally quantified at these
levels, but it's probably better to just think of them as free variables.
Anyway, the important think is that triples, sequents, and confgraphs are scope
boundaries for lvars.

The scope of a pvar is somewhat more complicated to describe. The intuition is
that the scope is the largest possible, as long as it talks about *one* point in
the execution. In a triple, the precondition and the postcondition are two
distinct scopes for pvars. In a sequent, the scope for pvars is the whole
sequent, just like for lvars. In a confgraph, the scope of pvars is one
node/configuration.

The implications for the semantics of triples, sequents, and confgraphs are the
following. A triple that contains lvars means that all triples that can be
obtained by substituting constants for lvars hold. In other words, we could say
that a triple means that all its instances with respect to lvars hold. Using
this language, a confgraph means that all its instances with respect to lvars
hold. (Just like in the case of the triple, the substitution lvar->constant is
applied for the *whole* confgraph.) Thus, once we have a semantics for a
confgraph without lvars, we'll also have a semantics for a confgraph with lvars.

Similarly, we could say that a sequent means that all its instantiations with
respect to lvars hold. This is nice and uniform with respect to the previous
paragraph, but it might be a bit misleading, for in a sequent, the distinction
pvar/lvar really isn't that important.


--------------------------------------------------------------------------------
-- Semantics of confgraphs

Confgraphs turn out to be a bit more beastly than I previously anticipated.
Given the previous section, we mostly need to worry about confgraphs that
mention no lvar. Up to a point. At the end we need to turn a confgraph into a
triple, and we don't want to do this at the level of instances (with repsect to
lvars).

Warning: The description here deviates a little from the current data-structure,
but that's because the data-structure is likely to change to conform to the
description from here.

OK. Enough warning. Let's start.

Flowgraphs and confgraphs are defined in the module Cfg.

A flowgraph is graph-based representation of the program being analyzed.
Vertices are labeled by commands. Arcs represent nondeterministic flow of
control, and are unlabelled. We maintain the invariant that each flowgraph has
exactly one distinguished entry vertex and one distinguished exit vertex, both
labelled by the command NOP.

While symbolically executing a flowgraph we build a confgraph. Intuitively, arcs
in the confgraph correspond to executing a command, so they should be labelled
by vertices in the underlying flowgraph, which in turn are labelled by commands.
However, the relation between a confgraph and its underlying flowgraph is a
little more loose, to allow for extra flexibility. First, the links between
flowgraph vertices and confgraph vertices are maintained in an interpreter
context. But, where these links are being kept is really not that important:
confgraphs are thrown away at the same time as interpreter contexts. Second,
only *some* vertices in the confgraph have links back to vertices in the
flowgraph. (These links are in [Symexec.ProcedureInterpreter.statement_of].)
Let's write s(c) for the command (aka statement) for which configuration c is a
post-configuration. Then, a path
  c0 → c1 → ... → cn
in the confgraph such that s(ck) is defined only for k=0 and k=n must occur only
if s(c0) → s(cn) is an arc in the flowgraph. Moreover the path c0→...→cn
corresponds to executing s(cn). (Consider the special case in which s(c) is
always defined. In that case n=1, always. Essentially each arc c0→c1 is labelled
by the command s(c1).) The extra flexibility (coming from allowing n>1) is used
as follows: c0→c1 simulates executing cn, but then c1→...→cn does some
simplifications. In any case, from now on I will pretend that we are in the
special case n=1, so that each arc in the confgraph is labelled by a vertex of
the flowgraph. This should make what follows more bearable, and shouldn't hide
anything important.

A confgraph is a digraph whose nodes are labeled by tuples (σ,H,t), and whose
arcs are labeled by tuples (C, M). Here,
  σ : Store  is the current store/stack, an assignment of constants to pvars
  H : Heap → 2  is a seplogic formula (‘Here’)
  t : {angelic, demonic} is a tag indicating the split type
  C  is a command in the program being analyzed
  M : Heap → 2 is a seplogic formula (‘Missing’)
where
  State = PVar → Val    (A value is a var-free term.)
(NOTE: To simplify the presentation, I assume that the command C comes from the
flowgraph as described in the previous paragraph, and I also assume that
confgraphs are structurally compatible with their underlying flowgraph, again as
described in the previous paragraph.) By decree, the formulas H and M do not
mention any pvars! If you feel like writing an H that mentions pvars, then just
apply the substitution σ to it. Imposing this invariant makes it trivial to
satisfy the side-condition of the frame rule later on.

Let us look now at the semantics of *one* arc in the confgraph. The arc
  (σ1,H1,_) ---(C, M)---> (σ2,H2,_)
intuitively corresponds to the triple
  {eq(σ1)*H1*M} C {eq(σ2)→H2}
where eq(σ) is a bunch of *-joined equalities of the form x=σx, for each pvar x
in the domain of σ. In words, if the current store is σ1 and the heap satisfies
H1*M, and if after executing command C the state/stack satisfies σ2, then the
heap (after executing C) satisfies H2. To be even more semantic (triples are
somewhat algebraic), let R(C) be the relation (on program states) that describes
command C. Also suppose x is the only pvar in the program. Then the arc above
means that
  R(C)(x1,x2) → x1=σ1(x)*H1*M → x2=σ2(x) → H2
is valid, where x1 and x2 are two variables. The generalization to multiple
pvars per program should be clear. In other words, the generalization to
arbitrary stores should be clear. On the other hand, perhaps I should spell out
the generalization to arbitrary states, which have a heap in addition to the
store (TODO).

(If you don't feel anxious to find out what's with the weird triple in the
paragraph above, then feel free to skip this one. I'm afraid that I need to
bring back lvars into discussion, temporarily, in order to motivate why the
semantics of an arc are chosen with * in pre and → in post. Consider a confgraph
with two arcs:
  (x=_x1,H1,_) --(C1,M1)--> (x=_x2,H2,_) --(C2,M2)--> (x=_x3,H3,_)
Here all formulas (H1, H2, H3, M2, M2) are pvar-free to make it easy to apply
the frame rule, but they do mention lvars _x1, _x2, _x3. Typically, H2 wouldn't
look into the future by mentioning _x3, but our semantics allows for that. Now,
we can instantiate this confgraph by picking values for _x1, _x2, and _x3. No
matter which values we pick, we should get something that accurately describes
the program being analyzed. If we happen to pick (_x1,_x2) ∈ R(C1), where R(C1)
is a relation describing the semantics of command C1, then the confgraph should
tell us something interesting about the program; if (_x1,_x2) ∉ R(C1), then the
confgraph should still be OK, but trivially. So, if we want the confgraph to be
translatable to a bunch of triples
  { x=_x1 * H1 * M1 } C1 { H2 }
  { x=_x2 * H2 * M2 } C2 { H3 }
we should put the equalities x=_x? in the pre, but not in the post. Suppose
_x1=1 and _x2=2, where we 1 and 2 come from a die, just because we can.  Now
instantiate the first triple: It says *if* x=1 and H1*M1, then H2. But, it
cannot say then x=2, because that 2 is completely arbitrary. In conclusion, we
want to *-join these equalities in the pre, but not in the post. Similarly, one
can motivate the implication part in the post, in order to account for Hs and Ms
that look into the future, although I believe we won't use that. In any case,
this is just an intuitive justification for why the semantics are chosen as the
previous paragraph stated.)

The previous convention for what an arc means composes nicely.
  (x=_x1,H1,_) --(C1,M1)--> (x=_x2,H2,_) --(C2,M2)--> (x=_x3,H3,_)
means that 
  ∀ _x1 _x2: { x=_x1 * H1 * M1 } C1 { x=_x2 → H2 }
  ∀ _x2 _x3: { x=_x2 * H2 * M2 } C2 { x=_x3 → H3 }
which imply
  ∀ _x1 _x3: { x=_x1 * H1 * M1 * M2 } C1;C2 { x=_x3 → H3 }
Sketch of how to check this: Write triples in the semantic way, using relations
R(C1), R(C2) and, respectively, R(C1;C2). Use the standard convention of what ;
means, namely that R(C1;C2)=R(C2)∘R(C1). Assume R(C1;C2), which means that an
intermediate state (_x2) exists. Then used the assumed triples, instantiating
_x2 to the thing that the previous sentence says it exists. This should yield
the conclusion.

OK. Now let's define the semantics of a path in the confgraph that starts at the
distinguished initial configuration. The semantics of the whole confgraph comes
later. For both these, we need to talk about the angelic/demonic tag.

Intuitively, a demonic split corresponds to a nondeterministic jump in the
program, and an angelic split corresponds to picking a triple (out of a set) for
a command. Thus, demonic splits always have (nop/goto, emp) as the label of
outgoing arcs, and angelic splits always have the same command on their outgoing
arcs but with possibly different Ms. In a sense, the confgraph encodes a game in
which the Devil tries to make the program go wrong by choosing which jumps to
make, and God chooses the triples (of individual commands) that are enough to
construct a proof.

The semantics of a path c0→c1→...→cn in the confgraph is that
  IF the two players (Devil and God) made the choices that pick this path
  THEN { x=_x0 * H0 * M1 * ... * Mn } C1;...;Cn { x=_xn → Hn }
Notice that a particular confgraph talks only about a slice of program behavior,
corresponding to the demonic choice made in order to go along this path. Thus,
it is not possible, in general, to infer something about the whole behavior of
the program from a single configuration. In particular, if we want to infer a
triple for the whole procedure we need to look at a *set* of confgraphs that are
each associated to the return statement of the procedure, and that together
cover all possible demonic choices.

Consider tests/infer_tests/test20.star. In one case, the body is
  call foo
where foo has three triples, {P1}(){Q1} and {P2}(){Q2} and {P3}(){Q3}.
Obviously, we'd like to conclude that the same spec (made of three triples) is
good for the procedure whose body is the single call above. In the other case,
the body is
  goto l1, l2, l3;
  l1: call foo1(); return;
  l2: call foo2(); return;
  l3: call foo3(); return;
where the spec of fook is {Pk}(){Qk}. What do we want to infer, ideally, in this
case? There are two possibilities that come two mind:
  { P1 ∧ P2 ∧ P3 } () { Q1 ∨ Q2 ∨ Q3 }
  { P1 * P2 * P3 } () { (Q1 * P2 * P2) ∨ (P1 * Q2 * P3) ∨ (P1 * P2 * Q3) }
The latter has the problem that it's likely that often P1*P2*P3 is inconsistent:
just consider an if-statement whose both branches read (x.f); that is, field f
of object x. The former has the problem that we don't have any support for ∧
yet: do we really want to interpret both ∧ and * everywhere (prover&symexec)?

Let's suppose we want something *like*
  { P1 ∧ P2 ∧ P3 } () { Q1 ∨ Q2 ∨ Q3 }
but expressed using only * and ∨. Let's focus on the precondition. Suppose Pk-s
are pure: then P1*P2*P3 works. Suppose now that, on the contrary, Pk-s have
(each) only a spatial predicate. Then we want a P such that P⊢P1 and P⊢P2 and
P⊢P3. Make it more concrete: P1 is x↦1; P2 is x↦_x1; P3 is x↦ls(x,_x2). Then we
want x↦1, because it entails (some instantiations) of all the others. The
general rule (that would handle ls and other predicates) seems to involve some
help from the user, perhaps just in the form of abstraction rules. It seems that
there's some solution where you ask the abduction question Pi*Aij?↦Pj for all i
and j and then use the answer to build a good P. However, I didn't work out the
details. This was proposed by NikosT. In contrast, in a previous discussion with
Rasmus, he expressed the belief that just detecting alpha-renamings might be
enough. That is, detect that given x↦_x1 and x↦_x2 you want just one (because an
alpha-renaming of lvars turns one into the other). The next step along these
lines is to say that usually P1=a*b and P2=c*d and if a and c are
alpha-convertible, then you want to say something like a*(b∧d) and continue
turning that b∧d conjunction into *-junction recursively.

Anyway ... I'll leave the above rather unpolished because the JACM paper on
bi-abduction uses another heuristic that may be simple to try first: Pretend we
don't know that it's not OK to get triples from single configurations and just
rely on the checking phase to weed out the bad triples. So, this would infer
{Pk}(){Qk} (for k=1,2,3) instead of the conjunctive version. Surely, these would
all be weeded out, no? Well, the JACM paper seems to suggest that this may
happen, but, if we do the inference on a transformed program then we get better
triples. The heuristic is called ‘assume as assert’. The idea is that a
nondeterministic split in the program is usually followed by an (assume φ). If
we transform the program and turn all assumes into asserts, then the
precondition of *one* configuration will insist that that path *must* be taken.
Which often gives what we want. So this is what I'll try first.

On the other hand, the version that says we should look at sets of confgraphs
that cover all demonic choices is somewhat theoretically luring, so I don't want
to bury that avenue for good. Just for now. Here are the interesting
subproblems. First, how do we enumerate all minimal sets of configurations that
cover all demonic choices? I know a reduction to the problem of enumerating
minimal unsatisfiable cores (with group 0). (That sounds cool, right? :) Second,
the fuzzy alpha-renaming and the abduction based approaches mentioned later need
some refinement, and they might give good results.


[TODO: Continue here.]

[1] http://youtu.be/IK0G04Mry3s


vim:spell:tw=80:fo+=t:
